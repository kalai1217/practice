{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52905c69-f92a-438d-b50d-60417b5fa80f",
     "showTitle": true,
     "title": "Read and Write CSV file "
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+--------------------+\n|Code|Symbol|                Name|\n+----+------+--------------------+\n| AED|   د.إ|United Arab Emira...|\n| AFN|     ؋|      Afghan afghani|\n| ALL|     L|        Albanian lek|\n| AMD|   AMD|       Armenian dram|\n| ANG|     ƒ|Netherlands Antil...|\n| AOA|    Kz|      Angolan kwanza|\n| ARS|     $|      Argentine peso|\n| AUD|     $|   Australian dollar|\n| AWG|  Afl.|       Aruban florin|\n| AZN|   AZN|   Azerbaijani manat|\n| BAM|    KM|Bosnia and Herzeg...|\n| BBD|     $|    Barbadian dollar|\n| BDT|    ৳ |    Bangladeshi taka|\n| BGN|   лв.|       Bulgarian lev|\n| BHD|  .د.ب|      Bahraini dinar|\n| BIF|    Fr|     Burundian franc|\n| BMD|     $|    Bermudian dollar|\n| BND|     $|       Brunei dollar|\n| BOB|   Bs.|  Bolivian boliviano|\n| BRL|    R$|      Brazilian real|\n+----+------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read and Write CSV\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read CSV file into DataFrame\n",
    "df = spark.read.csv(\"dbfs:/FileStore/tables/currency.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Display DataFrame\n",
    "df.show()\n",
    "\n",
    "\n",
    "# Write DataFrame to CSV file\n",
    "df.write.csv(\"dbfs:/path/to/output.csv\", header=True)\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a4805b8-44b6-4d7b-86d1-78b11f2c5f9b",
     "showTitle": true,
     "title": "Read and write JSON file"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- _corrupt_record: string (nullable = true)\n |-- array: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- dict: struct (nullable = true)\n |    |-- key: string (nullable = true)\n |-- int: long (nullable = true)\n |-- string: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"JSONReadWrite\").getOrCreate()\n",
    "\n",
    "# Read JSON file into DataFrame\n",
    "data_df = spark.read.json(\"dbfs:/FileStore/tables/multiline.json\")  \n",
    "\n",
    "# Print DataFrame schema (optional)\n",
    "data_df.printSchema()\n",
    "\n",
    "\n",
    "\n",
    "# Write DataFrame back to JSON file (overwrite mode)\n",
    "data_df.write.mode(\"overwrite\").json(\"dbfs:/path/to/output.json\")  \n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33795872-c9a1-4b6a-97ab-7f66ef05abaf",
     "showTitle": true,
     "title": "Read and write Parquet File"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n|              model| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|\n+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n|          Mazda RX4|21.0|  6|160.0|110| 3.9| 2.62|16.46|  0|  1|   4|   4|\n|      Mazda RX4 Wag|21.0|  6|160.0|110| 3.9|2.875|17.02|  0|  1|   4|   4|\n|         Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|\n|     Hornet 4 Drive|21.4|  6|258.0|110|3.08|3.215|19.44|  1|  0|   3|   1|\n|  Hornet Sportabout|18.7|  8|360.0|175|3.15| 3.44|17.02|  0|  0|   3|   2|\n|            Valiant|18.1|  6|225.0|105|2.76| 3.46|20.22|  1|  0|   3|   1|\n|         Duster 360|14.3|  8|360.0|245|3.21| 3.57|15.84|  0|  0|   3|   4|\n|          Merc 240D|24.4|  4|146.7| 62|3.69| 3.19| 20.0|  1|  0|   4|   2|\n|           Merc 230|22.8|  4|140.8| 95|3.92| 3.15| 22.9|  1|  0|   4|   2|\n|           Merc 280|19.2|  6|167.6|123|3.92| 3.44| 18.3|  1|  0|   4|   4|\n|          Merc 280C|17.8|  6|167.6|123|3.92| 3.44| 18.9|  1|  0|   4|   4|\n|         Merc 450SE|16.4|  8|275.8|180|3.07| 4.07| 17.4|  0|  0|   3|   3|\n|         Merc 450SL|17.3|  8|275.8|180|3.07| 3.73| 17.6|  0|  0|   3|   3|\n|        Merc 450SLC|15.2|  8|275.8|180|3.07| 3.78| 18.0|  0|  0|   3|   3|\n| Cadillac Fleetwood|10.4|  8|472.0|205|2.93| 5.25|17.98|  0|  0|   3|   4|\n|Lincoln Continental|10.4|  8|460.0|215| 3.0|5.424|17.82|  0|  0|   3|   4|\n|  Chrysler Imperial|14.7|  8|440.0|230|3.23|5.345|17.42|  0|  0|   3|   4|\n|           Fiat 128|32.4|  4| 78.7| 66|4.08|  2.2|19.47|  1|  1|   4|   1|\n|        Honda Civic|30.4|  4| 75.7| 52|4.93|1.615|18.52|  1|  1|   4|   2|\n|     Toyota Corolla|33.9|  4| 71.1| 65|4.22|1.835| 19.9|  1|  1|   4|   1|\n+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read and Write Parquet\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read Parquet file into DataFrame\n",
    "df = spark.read.parquet(\"dbfs:/FileStore/tables/MT_cars.parquet\")\n",
    "\n",
    "# Display DataFrame\n",
    "df.show()\n",
    "\n",
    "\n",
    "\n",
    "# Write DataFrame to Parquet file\n",
    "df.write.parquet(\"dbfs:/path/to/output.parquet\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "read_and_write_files",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
